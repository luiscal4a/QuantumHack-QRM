{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcffa560-5cc2-4431-9638-25ee63100c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38fa34-647e-4040-9c2c-cb94f289aa68",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fdd2fdc-42da-45df-8ee4-6bcdd1548d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"credit_risk_dataset_red_ml_ord.csv\")\n",
    "\n",
    "# scaled\n",
    "#df = pd.read_csv(\"credit_risk_dataset_red_ml_ord_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c9dbf5-19d1-4e4b-8a70-49c200430477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test set\n",
    "label = df['loan_status'] # labels\n",
    "features = df.drop('loan_status',axis=1) # features\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(features, label, \n",
    "                                                                    random_state=42, test_size=.30)\n",
    "\n",
    "x_all = features\n",
    "y_all = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964a75a-cda2-4910-9518-343e4a3bc4f3",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec00875-256b-4ce0-b946-13321b693245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_df(model, X, y):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame containing the actual labels, model predictions, \n",
    "    and the confidence (probability) of the predicted class.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A trained estimator (must have .predict(), ideally .predict_proba())\n",
    "    - X: Input features (DataFrame or array-like)\n",
    "    - y: True labels (Series or array-like)\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame with columns: ['y_true', 'y_pred', 'confidence']\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Generate Predictions\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # 2. Initialize the DataFrame\n",
    "    # We use valid indices from y if it's a pandas object, otherwise default\n",
    "    index = y.index if hasattr(y, 'index') else None\n",
    "    results_df = pd.DataFrame({\n",
    "        'y_true': y,\n",
    "        'y_pred': predictions\n",
    "    }, index=index)\n",
    "    \n",
    "    # 3. Calculate Confidence (if supported)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        # Get probabilities for all classes\n",
    "        probas = model.predict_proba(X)\n",
    "        \n",
    "        # Extract the maximum probability for each row (the confidence of the chosen class)\n",
    "        # axis=1 finds the max value across columns for each row\n",
    "        confidence = np.max(probas, axis=1)\n",
    "        \n",
    "        results_df['confidence'] = confidence\n",
    "    else:\n",
    "        # Fallback for models that don't support probabilities (e.g., SVM without probability=True)\n",
    "        results_df['confidence'] = np.nan\n",
    "        print(\"Warning: Model does not support 'predict_proba'. Confidence set to NaN.\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8feefd5-b11e-4546-9aaa-a0553414baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, X, y):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1, precision, recall, support, and confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained classification model (must have a .predict() method).\n",
    "    - X: The features to predict on.\n",
    "    - y: The true labels.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: A dictionary containing 'accuracy' and 'confusion_matrix'.\n",
    "    - report_df: A pandas DataFrame containing precision, recall, f1-score, and support per class.\n",
    "    \"\"\"\n",
    "    # 1. Generate Predictions\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # 2. Calculate Metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    # 3. Calculate Precision, Recall, F1, and Support\n",
    "    # precise_recall_fscore_support returns lists for each class\n",
    "    precision_l, recall_l, f1_l, support_l = precision_recall_fscore_support(y, y_pred)\n",
    "\n",
    "    # 4. Create a DataFrame for per-class metrics\n",
    "    # We attempt to use model.classes_ if available for row names, otherwise simple indices\n",
    "    try:\n",
    "        classes = model.classes_\n",
    "    except AttributeError:\n",
    "        classes = list(range(len(precision_l)))\n",
    "\n",
    "    report_df = pd.DataFrame({\n",
    "        'Precision': precision_l,\n",
    "        'Recall': recall_l,\n",
    "        'F1-Score': f1_l,\n",
    "        'Support': support_l\n",
    "    }, index=classes)\n",
    "\n",
    "    # 5. Calculate Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "    \n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # 6. ROC AUC Score\n",
    "    # Measures ability to distinguish between classes. 1.0 is perfect, 0.5 is random guessing.\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "\n",
    "    # Bundle everything into a dictionary\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "\n",
    "\n",
    "    print(report_df)\n",
    "\n",
    "    for key in metrics.keys():\n",
    "        print(key, metrics[key])\n",
    "        \n",
    "    print(conf_matrix)\n",
    "\n",
    "    prediction_df = create_prediction_df(model, X, y)\n",
    "\n",
    "    df_confidence = pd.concat([X, prediction_df['confidence']], axis=1)\n",
    "\n",
    "    return metrics, report_df, prediction_df, df_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12afcdc3-ba0b-4061-b3d1-5fe98ab6bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_stats(df):\n",
    "    \"\"\"\n",
    "    Returns a summary table with mean, min, and max confidence \n",
    "    for correct vs. incorrect predictions.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_stats = df.copy()\n",
    "    \n",
    "    # Create a boolean column for grouping\n",
    "    df_stats['prediction_status'] = df_stats.apply(\n",
    "        lambda row: 'Correct' if row['y_true'] == row['y_pred'] else 'Incorrect', \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Group by the status and calculate statistics on the 'confidence' column\n",
    "    summary = df_stats.groupby('prediction_status')['confidence'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed74014-f46b-4e32-be2f-4a442f88dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_x_confidence(df, x):\n",
    "\n",
    "    return df.nsmallest(x, 'confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f802e9f-6a52-4e0a-807f-e43d9a82c0a0",
   "metadata": {},
   "source": [
    "## XGBoost train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a0b1a6-162f-4d58-8663-593bac7606eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precision    Recall  F1-Score  Support\n",
      "0   0.976131  0.993606  0.984791     2346\n",
      "1   0.975490  0.912844  0.943128      654\n",
      "accuracy 0.976\n",
      "Precision 0.9754901960784313\n",
      "Recall 0.9128440366972477\n",
      "F1-Score 0.943127962085308\n",
      "confusion_matrix [[2331   15]\n",
      " [  57  597]]\n",
      "roc_auc 0.9801829387518869\n",
      "[[2331   15]\n",
      " [  57  597]]\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42) \n",
    "\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Substitute as you want x_train, y_train; x_test, y_test; x_all y_all\n",
    "metrics, report_df, prediction_df, df_confidence = evaluate_classification(xgb_model, x_all, y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43e185-ba00-4bfd-9229-41a7d9852470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "selected_features = [\n",
    "    \"person_home_ownership_RENT\",\n",
    "    \"loan_grade\",\n",
    "    \"loan_percent_income\",\n",
    "    \"person_home_ownership_OWN\",\n",
    "    \"person_home_ownership_MORTGAGE\",\n",
    "    \"loan_intent_DEBTCONSOLIDATION\",\n",
    "    \"loan_int_rate\",\n",
    "    \"loan_intent_HOMEIMPROVEMENT\",\n",
    "    \"loan_intent_MEDICAL\",\n",
    "    \"loan_amnt\",\n",
    "    \"loan_status\"\n",
    "]\n",
    "\n",
    "update_df = df[selected_features]\n",
    "df_confidence_prova = pd.concat([update_df, prediction_df['confidence']], axis=1)\n",
    "print(len(df_confidence_prova))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e5c4a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_status\n",
       "0    142\n",
       "1    142\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_points_from_which_to_balance = 300\n",
    "\n",
    "lowest = lowest_x_confidence(df_confidence_prova, number_of_points_from_which_to_balance)\n",
    "# THIS IS UNBALANCED \n",
    "\n",
    "counts = lowest[\"loan_status\"].value_counts()\n",
    "n_min = counts.min()\n",
    "\n",
    "lowest_balanced = ( \n",
    "    pd.concat([\n",
    "        lowest[lowest[\"loan_status\"] == 0 ].sample(n = n_min, random_state=42), \n",
    "        lowest[lowest[\"loan_status\"] == 1 ].sample(n = n_min, random_state=42), \n",
    "        ])\n",
    "        .sample(frac=1, random_state = 42)\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "lowest_balanced[\"loan_status\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72d1c155-a482-425d-982e-ca052d921f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_balanced.to_csv('name_of_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6902949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_status\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_paolo = lowest_balanced.drop(columns=['loan_status', 'confidence']).values\n",
    "y_paolo = lowest_balanced['loan_status'].values\n",
    "\n",
    "print(lowest_balanced['loan_status'].value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split by percentage\n",
    "train_percentage = 0.8  # 80% train, 20% test\n",
    "X_train, X_test, train_labels, test_labels = train_test_split(\n",
    "    X_paolo, y_paolo, train_size=train_percentage, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "import xgboost\n",
    "xgb_model = xgboost.XGBClassifier(objective = \"binary:logistic\", random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, train_labels)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e494fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.53125\n",
      "Recall: 0.6071428571428571\n",
      "F1: 0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision:\", precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\", recall_score(test_labels, y_pred))\n",
    "print(\"F1:\", f1_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb52fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
